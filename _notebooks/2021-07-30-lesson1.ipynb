{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de3ae24",
   "metadata": {
    "id": "8a3622fa"
   },
   "source": [
    "# Lesson 1 Notes\n",
    "> notes for the fast.ai 2020 course\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastai course]\n",
    "- image: images/fastai.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93beea9d",
   "metadata": {},
   "source": [
    "## What you don't need, to deep learning\n",
    "Myth(don't need)| Truth\n",
    ":---------------|:------\n",
    "Lots of math    | Just high school math is sufficient\n",
    "Lots of data    | We've seen record-breaking results with <50 items of data\n",
    "Lots of expensive computers | You can get what you need for state of the art work for free\n",
    "\n",
    "A lot of world class research projects have come out of the fastai students based on a single GPU, using small data or without a traditional background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26129e32",
   "metadata": {},
   "source": [
    "## Where is deep learning the best known approach?\n",
    "![](../images/AI_the_best.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11b4cd",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "##### The Start\n",
    "In 1943, Warren McCulloch and Walter Pitts developed a mathematical model of an artificial neuron.\n",
    "\n",
    "In 1957, Frank Rosenblatt built the first device that actually used these principles, *the Mark I Perceptron* at Cornell. \n",
    "> We are now about to witness the birth of such a machine–-a machine capable of perceiving, recognizing and identifying its surroundings without any human training or control. \n",
    ">\n",
    "> ---- <cite>Frank Rosenblatt</cite>\n",
    "\n",
    "##### The First AI Winter\n",
    "1969, Marvin Minsky adn Seymour Papert wrote a book called *Perceptrons* and pointed out that *a single layer of a NN cannot learn some simple but critical functions (such as XOR) and using multiple layers of the devices would allow these limitations to be addressed*. Unfortunately, only the first of these insights was widely recognized.\n",
    "##### The Second Winter\n",
    "In theory, adding just one extra layer of neurons was enough to allow any mathematical function to be approximated with these neural networks, but in practice such networks were often too big and too slow to be useful. Although researchers showed 30 years ago that to get practical good performance you need to use even more layers of neurons, it is only in the last decade that this principle has been more widely appreciated and applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9cc79c",
   "metadata": {
    "id": "33fde6f9"
   },
   "source": [
    "## The fast.ai Learning Philosophy\n",
    "![](../images/bat.jpg)\n",
    "\n",
    "##### Projects and Mindset\n",
    "It helps to focus on your hobbies and passions–-setting yourself four or five little projects rather than striving to solve a big, grand problem tends to work better when you're getting started.\n",
    "\n",
    "##### Questionnaire\n",
    "After every section there are questionnaire which makes sure that you learned the most important things. It doesn't matter how many you get right but it just confirms that you haven't missed anything important. If you don't understand something after some time just continue and come back after a few chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f26b92",
   "metadata": {},
   "source": [
    "## The Software: PyTorch, fastai, and Jupyter\n",
    "fastai is built top of PyTorch and these are written in Python and it's the language we will use during this  course. Many people think that fastai is just for beginners and teachers but it's actually using layered API which makes it infinite customizable and practical for every purpose.\n",
    "\n",
    "[Jupyter Notebook](https://jupyter.org/) is coding environment often used by DL people. It's easier to experiment things using Jupyter Notebooks than running Python code in terminal. Linux highly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "\n",
    "from fastbook import *\n",
    "\n",
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "\n",
    "def is_cat(x): return x[0].isupper()\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(224))\n",
    "\n",
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e830e7c",
   "metadata": {},
   "source": [
    "## What Is Machine Learning?\n",
    "*The training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.*\n",
    "\n",
    "In 1949, an IBM researcher named Arthur Samuel started working on machine learning. \n",
    "\n",
    "By 1961 his checkers-playing program beat the Connecticut state champion.\n",
    "\n",
    "In his classic 1962 essay \"Artificial Intelligence: A Frontier of Automation\", he wrote:\n",
    "> Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would \"learn\" from its experience.\n",
    "\n",
    "\n",
    "* the effectiveness → Loss \n",
    "* weight assignment → parameters of Neural Networks\n",
    "* performance → predictions\n",
    "* mechanism for altering the weight assignment → stochastic gradient descent (SGD)\n",
    "* maximize the performance → Minimize the Loss\n",
    "![](../images/ml.png)\n",
    "\n",
    "##### Limitations Inherent To Machine Learning\n",
    "* A model cannot be created without data.\n",
    "* A model can only learn to operate on the patterns seen in the input data used to train it.\n",
    "* This learning approach only creates predictions, not recommended actions.\n",
    "* It's not enough to just have examples of input data; we need labels for that data too\n",
    "\n",
    "##### Classification and Regression:\n",
    "* A classification model is one which attempts to predict a class, or category\n",
    "* A regression model is one which attempts to predict one or more numeric quantities, such as a temperature or a location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf222f5",
   "metadata": {},
   "source": [
    "## Only Avoid Overfitting When Needed\n",
    "We often see practitioners using over-fitting avoidance techniques even when they have enough data that they didn't need to do so, ending up with a model that may be less accurate than what they could have achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b3710",
   "metadata": {},
   "source": [
    "## Jargon Recap\n",
    "Term  | Meaning\n",
    ":-----|:-------\n",
    "Label | The data that we're trying to predict, such as \"dog\" or \"cat\"\n",
    "Architecture | The _template_ of the model that we're trying to fit; the actual mathematical function that we're passing the input data and parameters to\n",
    "Model | The combination of the architecture with a particular set of parameters\n",
    "Parameters | The values in the model that change what task it can do, and are updated through model training\n",
    "Fit | Update the parameters of the model such that the predictions of the model using the input data match the target labels\n",
    "Train | A synonym for _fit_\n",
    "Pretrained model | A model that has already been trained, generally using a large dataset, and will be fine-tuned\n",
    "Fine-tune | Update a pretrained model for a different task\n",
    "Transfer learning | Using a pretrained model for a task different to what it was originally trained for\n",
    "Epoch | One complete pass through the input data\n",
    "Loss | A measure of how good the model is, chosen to drive training via SGD\n",
    "Metric | A measurement of how good the model is, using the validation set, chosen for human consumption\n",
    "Validation set | A set of data held out from training, used only for measuring how good the model is\n",
    "Training set | The data used for fitting the model; does not include any data from the validation set\n",
    "Overfitting | Training a model in such a way that it _remembers_ specific features of the input data, rather than generalizing well to data not seen during training\n",
    "CNN | Convolutional neural network; a type of neural network that works particularly well for computer vision tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dfca7d",
   "metadata": {},
   "source": [
    "## Before the next lesson:\n",
    "\n",
    "- Setup all tools\n",
    "- Get comfortable with the notebook, docs, and style of writing Python"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2021-07-18-myth.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
