{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c771ec",
   "metadata": {
    "id": "8a3622fa"
   },
   "source": [
    "# Lesson 1 Notes\n",
    "> notes for the fast.ai 2020 course\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastai course]\n",
    "- image: images/fastai.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ab9c0",
   "metadata": {},
   "source": [
    "## What you don't need, to deep learning\n",
    "Myth(don't need)| Truth\n",
    ":---------------|:------\n",
    "Lots of math    | Just high school math is sufficient\n",
    "Lots of data    | We've seen record-breaking results with <50 items of data\n",
    "Lots of expensive computers | You can get what you need for state of the art work for free\n",
    "\n",
    "A lot of world class research projects have come out of the fastai students based on a **single GPU**, using **small data** or without a **traditional background**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2706669",
   "metadata": {},
   "source": [
    "## Where is deep learning the best known approach?\n",
    "![](../images/AI_the_best.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d0efa",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "### The Start\n",
    "* In 1943, Warren McCulloch and Walter Pitts developed a mathematical model of an artificial neuron.\n",
    "* In 1957, Frank Rosenblatt built the first device that actually used these principles, **the Mark I Perceptron** at Cornell. \n",
    ">We are now about to witness the birth of such a machine–-a machine capable of perceiving, recognizing and identifying its surroundings without any human training or control. \n",
    ">\n",
    ">-- <cite>Frank Rosenblatt</cite>\n",
    "\n",
    "### The First AI Winter\n",
    "1969, Marvin Minsky adn Seymour Papert wrote a book called *Perceptrons* and pointed out that **a single layer of a NN cannot learn some simple but critical functions (such as XOR) and using multiple layers of the devices would allow these limitations to be addressed**. Unfortunately, only the first of these insights was widely recognized.\n",
    "### The Second Winter\n",
    "In theory, adding just one extra layer of neurons was enough to allow any mathematical function to be approximated with these neural networks, but in practice such networks were often too big and too slow to be useful. Although researchers showed 30 years ago that to get practical good performance you need to use even more layers of neurons, it is only in the last decade that this principle has been more widely appreciated and applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5be46c",
   "metadata": {
    "id": "33fde6f9"
   },
   "source": [
    "## The fast.ai Learning Philosophy\n",
    "![](../images/bat.jpg)\n",
    "\n",
    "## Your Projects and Your Mindset\n",
    "It helps to focus on your hobbies and passions–-setting yourself four or five little projects rather than striving to solve a big, grand problem tends to work better when you're getting started."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2021-07-18-myth.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
